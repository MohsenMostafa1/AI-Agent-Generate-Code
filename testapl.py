# -*- coding: utf-8 -*-
"""TestAPL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xvgRNVPKIGeEZMwzTjoQH3okpVEVKx03
"""

!pip install -q torch transformers sentence-transformers hnswlib fastapi uvicorn ctransformers docker psutil pyngrok nest-asyncio llama-cpp-python optuna scikit-opt autogluon.core trl

!wget https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q6_K.gguf

# -*- coding: utf-8 -*-
"""APL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hw5LD7ibVVlmhdz7Pg5HymYh7ABCHzud
"""


# -*- coding: utf-8 -*-
"""Advanced Code Generation Tool with Feedback and Function Calling Integration"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging
import warnings
warnings.filterwarnings("ignore")

# Import all required libraries
from pyngrok import ngrok
import nest_asyncio
import uvicorn
import threading
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import hnswlib
import logging
import aiohttp
from fastapi.middleware.cors import CORSMiddleware
import json
import os
import time  # Add this with other imports
from bs4 import BeautifulSoup
import requests
import numpy as np
import sqlite3
from datetime import datetime
import subprocess
from typing import List, Dict, Optional, Union, Callable, Any
import hashlib
import re
from pathlib import Path
import psutil
from llama_cpp import Llama
from huggingface_hub import notebook_login
from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from enum import Enum
import optuna
from optuna.trial import Trial
from optuna.samplers import TPESampler
from optuna.distributions import FloatDistribution as Real, IntDistribution as Integer
import torch
from typing import Optional, List, Dict
from fastapi import HTTPException
from transformers import StoppingCriteria, StoppingCriteriaList
import hashlib
import json
import logging
from datetime import datetime


nest_asyncio.apply()

# Disable Docker in Colab by default
USE_DOCKER = False


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI with CORS
app = FastAPI(title="Advanced Code Generator with Feedback and Function Calling",
             description="API for generating and executing code with feedback-driven learning and function calling",
             version="3.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Configuration Class ---
class Config:
    # Architecture Optimization
    AUTOTUNE_INTERVAL = 100
    NAS_SEARCH_SPACE = {
        'retriever_M': Integer(10, 64),
        'retriever_ef': Integer(100, 500),
        'model_temp': Real(0.1, 1.0),
        'model_top_p': Real(0.5, 1.0)
    }
    RLHF_CONFIG = {
        'learning_rate': 2e-5,
        'batch_size': 8,
        'num_epochs': 3
    }

    # Vector Retrieval
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
    RETRIEVER_MAX_ITEMS = 10000
    RETRIEVER_EF_CONSTRUCTION = 200
    RETRIEVER_M = 16

    # Model Configuration
    MODEL_NAME = "deepseek-ai/deepseek-coder-6.7b-instruct"
    MODEL_PATH = "deepseek-coder-6.7b-instruct.Q6_K.gguf"
    MODEL_CONFIG = {
        "torch_dtype": torch.float16,
        "device_map": "auto",
        "low_cpu_mem_usage": True
    }

    # Generation Parameters
    GENERATION_CONFIG = {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True,
        "repetition_penalty": 1.1
    }

    # Function Calling
    FUNCTION_CALL_PREFIX = "<function>"
    FUNCTION_CALL_SUFFIX = "</function>"
    MAX_FUNCTION_CALL_ATTEMPTS = 3
    FUNCTION_CALL_TOKEN_LIMIT = 2000

    # Supported Languages
    SUPPORTED_LANGUAGES = {
        "python": {
            "ext": ".py",
            "docker_image": "python:3.9-slim",
            "validation_patterns": [r"def\s+\w+", r"import\s+\w+"],
            "linter": "flake8",
            "categories": ["web", "data", "ml"]
        },
        # ... (other language configurations remain the same)
    }

    # Memory & History
    HISTORY_DB = "code_history.db"
    MAX_HISTORY_ITEMS = 1000
    KNOWLEDGE_BASE_LIMIT = 15
    CONTEXT_WINDOW = 5  # Number of historical items for context

    # Code Execution
    DOCKER_TIMEOUT = 30  # seconds
    MAX_OUTPUT_LENGTH = 1000  # characters

    # Feedback System
    FEEDBACK_WEIGHT = 0.7
    USAGE_WEIGHT = 0.3
    DECAY_FACTOR = 0.95
    MIN_FEEDBACK_SCORE = 0.1
    FEEDBACK_THRESHOLD = 0.7

def load_gguf_model():
    """GGUF model loader (now outside Config class)"""
    logger.info(f"Loading GGUF model: {Config.MODEL_PATH}")
    try:
        return Llama(
            model_path=Config.MODEL_PATH,
            n_ctx=2048,
            n_threads=4,
            n_gpu_layers=33 if torch.cuda.is_available() else 0,
            verbose=False
        )
    except Exception as e:
        logger.error(f"GGUF model loading failed: {e}")
        raise RuntimeError(f"Could not load GGUF model: {str(e)}")


    # Model Configuration
    MODEL_NAME = "deepseek-ai/deepseek-coder-6.7b-instruct"
    MODEL_PATH = "deepseek-coder-6.7b-instruct.Q6_K.gguf"
    MODEL_CONFIG = {
        "torch_dtype": torch.float16,
        "device_map": "auto",
        "low_cpu_mem_usage": True
    }

    # Generation Parameters
    GENERATION_CONFIG = {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True,
        "repetition_penalty": 1.1
    }

    # Function Calling Configuration
    FUNCTION_CALL_PREFIX = "<function>"
    FUNCTION_CALL_SUFFIX = "</function>"
    MAX_FUNCTION_CALL_ATTEMPTS = 3
    FUNCTION_CALL_TOKEN_LIMIT = 2000

    # Supported Languages Configuration
    SUPPORTED_LANGUAGES = {
        "python": {
            "ext": ".py",
            "docker_image": "python:3.9-slim",
            "validation_patterns": [r"def\s+\w+", r"import\s+\w+"],
            "linter": "flake8",
            "categories": ["web", "data", "ml"]
        },
        "javascript": {
            "ext": ".js",
            "docker_image": "node:18-slim",
            "validation_patterns": [r"function\s+\w+", r"const\s+\w+", r"let\s+\w+"],
            "linter": "eslint",
            "categories": ["web", "server", "frontend"]
        },
        "typescript": {
            "ext": ".ts",
            "docker_image": "node:18-slim",
            "validation_patterns": [r"function\s+\w+", r"const\s+\w+", r"interface\s+\w+"],
            "linter": "eslint",
            "categories": ["web", "server", "frontend"]
        },
        "java": {
            "ext": ".java",
            "docker_image": "openjdk:17-jdk-slim",
            "validation_patterns": [r"class\s+\w+", r"public\s+\w+\s+\w+\s*\("],
            "linter": "checkstyle",
            "categories": ["backend", "android"]
        },
        "react": {
            "ext": ".jsx",
            "docker_image": "node:18-slim",
            "validation_patterns": [r"function\s+\w+", r"export\s+default", r"<\w+"],
            "linter": "eslint",
            "categories": ["frontend", "web"]
        },
        "tailwind": {
            "ext": ".jsx",
            "docker_image": "node:18-slim",
            "validation_patterns": [r"className=\"[^\"]*"],
            "linter": None,
            "categories": ["frontend", "css"]
        },
        "vite": {
            "ext": ".js",
            "docker_image": "node:18-slim",
            "validation_patterns": [r"import\s+\w+", r"createApp\("],
            "linter": "eslint",
            "categories": ["frontend", "build"]
        },
        "nestjs": {
            "ext": ".ts",
            "docker_image": "node:18-slim",
            "validation_patterns": [r"@\w+\(\)", r"class\s+\w+"],
            "linter": "eslint",
            "categories": ["backend", "server"]
        }
    }

    # Memory & History Configuration
    HISTORY_DB = "code_history.db"
    MAX_HISTORY_ITEMS = 1000
    KNOWLEDGE_BASE_LIMIT = 15
    CONTEXT_WINDOW = 5  # Number of historical items to consider for context

    # Code Execution Configuration
    DOCKER_TIMEOUT = 30  # seconds
    MAX_OUTPUT_LENGTH = 1000  # characters

    # Feedback and Ranking Configuration
    FEEDBACK_WEIGHT = 0.7  # Weight for feedback scores in ranking
    USAGE_WEIGHT = 0.3    # Weight for usage frequency in ranking
    DECAY_FACTOR = 0.95   # Decay factor for older feedback
    MIN_FEEDBACK_SCORE = 0.1  # Minimum score for low-quality items
    FEEDBACK_THRESHOLD = 0.7   # Score threshold for considering feedback positive

# Automated Architecture Optimization
class ArchitectureOptimizer:
    def __init__(self, retriever: 'FeedbackAwareRetriever'):
        self.retriever = retriever
        self.study = optuna.create_study(direction='minimize')

    def _evaluate_retriever_performance(self) -> float:
        """Evaluate retriever performance using a simple test query"""
        try:
            test_query = "python function example"
            results = self.retriever.search_code(test_query)
            if not results:
                return 0.0

            # Score based on average combined score of top results
            avg_score = sum(r['combined_score'] for r in results) / len(results)
            # We want to minimize (1 - score) since optuna minimizes the objective
            return 1.0 - avg_score
        except Exception as e:
            logger.error(f"Evaluation failed: {str(e)}")
            return 1.0  # Worst possible score if evaluation fails

    def _objective(self, trial):
        M = trial.suggest_int('M', 10, 64)
        ef_construction = trial.suggest_int('ef_construction', 100, 500)

        try:
            # Create a new temporary index with the trial parameters
            temp_index = hnswlib.Index(space='cosine', dim=self.retriever.index.dim)
            temp_index.init_index(
                max_elements=Config.RETRIEVER_MAX_ITEMS,
                ef_construction=ef_construction,
                M=M
            )

            # Copy embeddings to the temp index (simplified - in reality you'd need proper index copying)
            if hasattr(self.retriever.index, 'get_items'):
                ids, embeddings = self.retriever.index.get_items()
                temp_index.add_items(embeddings, ids)

            # Temporarily replace the retriever's index
            original_index = self.retriever.index
            self.retriever.index = temp_index

            # Evaluate performance
            score = self._evaluate_retriever_performance()

            # Restore original index
            self.retriever.index = original_index

            return score
        except Exception as e:
            raise optuna.TrialPruned(f"Index initialization failed: {str(e)}")

    def optimize(self, n_trials=50):
        self.study.optimize(self._objective, n_trials=n_trials)
        best_params = self.study.best_params

        # Reinitialize the main index with best parameters
        self.retriever.index.init_index(
            max_elements=Config.RETRIEVER_MAX_ITEMS,
            ef_construction=best_params['ef_construction'],
            M=best_params['M']
        )

class GenerationParameterTuner:
    def __init__(self, agent):
        self.agent = agent
        self.study = optuna.create_study(direction='maximize')

    def optimize(self, n_trials=100):
        def objective(trial):
            temp = trial.suggest_float('temp', *Config.NAS_PARAMS['model_temp_range'])
            top_p = trial.suggest_float('top_p', *Config.NAS_PARAMS['model_top_p_range'])

            # Temporary override
            original_temp = self.agent.model.generation_config.temperature
            original_top_p = self.agent.model.generation_config.top_p

            self.agent.model.generation_config.temperature = temp
            self.agent.model.generation_config.top_p = top_p

            # Evaluate quality
            score = self._evaluate_generation_quality()

            # Restore original
            self.agent.model.generation_config.temperature = original_temp
            self.agent.model.generation_config.top_p = original_top_p

            return score

        self.study.optimize(objective, n_trials=n_trials)
        best = self.study.best_params
        self.agent.model.generation_config.temperature = best['temp']
        self.agent.model.generation_config.top_p = best['top_p']

# --- Function Calling System ---
class FunctionCallingSystem:
    class ToolType(Enum):
        CODE_EXECUTION = "execute_code"
        CODE_LINTING = "lint_code"
        KNOWLEDGE_RETRIEVAL = "get_knowledge"
        CODE_RETRIEVAL = "search_code"
        HISTORY_RETRIEVAL = "get_history"

    def __init__(self, executor: 'CodeExecutor', memory: 'CodeMemory', retriever: 'FeedbackAwareRetriever'):
        self.executor = executor
        self.memory = memory
        self.retriever = retriever
        self.available_tools = self._initialize_tools()

    def _initialize_tools(self) -> Dict[str, Dict]:
        """Initialize available tools with their metadata and functions"""
        return {
            self.ToolType.CODE_EXECUTION.value: {
                "description": "Execute code in a sandboxed environment and return the output",
                "parameters": {
                    "code": {"type": "string", "description": "The code to execute"},
                    "language": {"type": "string", "description": "Programming language of the code"}
                },
                "function": self._execute_code
            },
            self.ToolType.CODE_LINTING.value: {
                "description": "Lint code to check for syntax errors and style violations",
                "parameters": {
                    "code": {"type": "string", "description": "The code to lint"},
                    "language": {"type": "string", "description": "Programming language of the code"}
                },
                "function": self._lint_code
            },
            self.ToolType.KNOWLEDGE_RETRIEVAL.value: {
                "description": "Retrieve relevant knowledge base entries for a given language and category",
                "parameters": {
                    "language": {"type": "string", "description": "Programming language to filter by"},
                    "category": {"type": "string", "description": "Optional category to filter by"},
                    "query": {"type": "string", "description": "Optional search query"},
                    "limit": {"type": "integer", "description": "Maximum number of results to return"}
                },
                "function": self._retrieve_knowledge
            },
            self.ToolType.CODE_RETRIEVAL.value: {
                "description": "Search for relevant code snippets based on a query",
                "parameters": {
                    "query": {"type": "string", "description": "Search query"},
                    "language": {"type": "string", "description": "Programming language to filter by"},
                    "category": {"type": "string", "description": "Optional category to filter by"},
                    "limit": {"type": "integer", "description": "Maximum number of results to return"}
                },
                "function": self._retrieve_code
            },
            self.ToolType.HISTORY_RETRIEVAL.value: {
                "description": "Retrieve relevant historical code generations",
                "parameters": {
                    "language": {"type": "string", "description": "Programming language to filter by"},
                    "category": {"type": "string", "description": "Optional category to filter by"},
                    "limit": {"type": "integer", "description": "Maximum number of results to return"}
                },
                "function": self._retrieve_history
            }
        }

    async def call_tool(self, tool_name: str, parameters: Dict) -> Dict:
        """Execute a tool with the given parameters"""
        if tool_name not in self.available_tools:
            raise ValueError(f"Unknown tool: {tool_name}")

        tool = self.available_tools[tool_name]
        try:
            result = await tool["function"](**parameters)
            return {
                "tool": tool_name,
                "success": True,
                "result": result,
                "error": None
            }
        except Exception as e:
            return {
                "tool": tool_name,
                "success": False,
                "result": None,
                "error": str(e)
            }

    async def _execute_code(self, code: str, language: str) -> Dict:
        """Execute code using the code executor"""
        return self.executor.execute_code(code, language)

    async def _lint_code(self, code: str, language: str) -> Dict:
        """Lint code using the code executor"""
        return self.executor.lint_code(code, language)

    async def _retrieve_knowledge(self, language: str, category: Optional[str] = None,
                                query: Optional[str] = None, limit: int = 5) -> List[Dict]:
        """Retrieve knowledge base entries"""
        if query:
            # If we have a query, use the retriever
            results = self.retriever.search_code(query, language_filter=language, category_filter=category)
            return [{"content": r["code"], "source": r["source"], "score": r["combined_score"]} for r in results[:limit]]
        else:
            # Otherwise use the memory system
            return self.memory.get_knowledge(language=language, category=category, limit=limit)

    async def _retrieve_code(self, query: str, language: Optional[str] = None,
                           category: Optional[str] = None, limit: int = 5) -> List[Dict]:
        """Retrieve relevant code snippets"""
        results = self.retriever.search_code(query, language_filter=language, category_filter=category)
        return [{
            "code": r["code"],
            "source": r["source"],
            "language": r["language"],
            "category": r.get("category", ""),
            "score": r["combined_score"]
        } for r in results[:limit]]

    async def _retrieve_history(self, language: str, category: Optional[str] = None,
                              limit: int = 5) -> List[Dict]:
        """Retrieve historical code generations"""
        return self.memory.get_best_history(language=language, category=category, limit=limit)

    def get_tools_prompt(self) -> str:
        """Generate a prompt section describing available tools"""
        tools_desc = []
        for tool_name, tool_info in self.available_tools.items():
            params = "\n".join([f"    - {name}: {desc['description']}"
                              for name, desc in tool_info["parameters"].items()])
            tools_desc.append(
                f"- {tool_name}: {tool_info['description']}\n"
                f"  Parameters:\n{params}"
            )
        return "Available Tools:\n" + "\n".join(tools_desc)

    def detect_function_call(self, text: str) -> Optional[Dict]:
        """
        Detect and parse a function call in the model's output.
        Format: <function>tool_name</function>{parameters}
        """
        pattern = re.compile(
            rf"{re.escape(Config.FUNCTION_CALL_PREFIX)}(.*?){re.escape(Config.FUNCTION_CALL_SUFFIX)}(.*)",
            re.DOTALL
        )
        match = pattern.search(text)
        if not match:
            return None

        tool_name = match.group(1).strip()
        try:
            parameters = json.loads(match.group(2).strip())
            return {"tool": tool_name, "parameters": parameters}
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse function parameters: {match.group(2)}")
            return None


# --- Feedback-Aware Vector Retrieval System ---
class FeedbackAwareRetriever:
    def __init__(self, vector_dim=384):
        self.index = hnswlib.Index(space='cosine', dim=vector_dim)
        self.index.init_index(
            max_elements=Config.RETRIEVER_MAX_ITEMS,
            ef_construction=Config.RETRIEVER_EF_CONSTRUCTION,
            M=Config.RETRIEVER_M
        )
        self.code_snippets = []
        self.metadata = []
        self.id_counter = 0
        self.feedback_scores = {}  # Track feedback scores for each snippet
        self.usage_counts = {}     # Track usage counts for each snippet
        self.last_used = {}        # Track when snippets were last used
        self.architecture_optimizer = ArchitectureOptimizer(self)
        self.optimization_count = 0

    def auto_optimize(self):
        if self.optimization_count % Config.AUTOTUNE_INTERVAL == 0:
            self.retriever.architecture_optimizer.optimize()
        self.optimization_count += 1

    def add_code(self, code: str, source: str = "", language: str = "", category: str = ""):
        embedding = self._embed(code)
        self.code_snippets.append(code)
        self.metadata.append({
            "source": source,
            "language": language,
            "category": category,
            "timestamp": datetime.now().isoformat()
        })
        self.index.add_items([embedding], [self.id_counter])
        self.feedback_scores[self.id_counter] = 1.0  # Initial neutral score
        self.usage_counts[self.id_counter] = 0       # Initial usage count
        self.last_used[self.id_counter] = datetime.now()
        self.id_counter += 1

    def update_feedback(self, snippet_id: int, feedback_score: float):
        """Update feedback score for a code snippet with decay factor"""
        if snippet_id in self.feedback_scores:
            current_score = self.feedback_scores[snippet_id]
            # Apply decay to existing feedback and add new feedback
            self.feedback_scores[snippet_id] = max(
                Config.MIN_FEEDBACK_SCORE,
                Config.DECAY_FACTOR * current_score +
                (1 - Config.DECAY_FACTOR) * feedback_score
            )

    def increment_usage(self, snippet_id: int):
        """Increment usage count for a code snippet"""
        if snippet_id in self.usage_counts:
            self.usage_counts[snippet_id] += 1
            self.last_used[snippet_id] = datetime.now()

    def get_combined_score(self, snippet_id: int) -> float:
        """Calculate combined score based on feedback and usage"""
        if snippet_id not in self.feedback_scores or snippet_id not in self.usage_counts:
            return 0.0

        # Normalize usage count (log scale to prevent dominance)
        max_usage = max(self.usage_counts.values()) if self.usage_counts else 1
        normalized_usage = np.log1p(self.usage_counts[snippet_id]) / np.log1p(max_usage)

        return (
            Config.FEEDBACK_WEIGHT * self.feedback_scores[snippet_id] +
            Config.USAGE_WEIGHT * normalized_usage
        )

    def search_code(self, query: str, top_k: int = 5,
                   language_filter: Optional[str] = None,
                   category_filter: Optional[str] = None) -> List[Dict]:
        if not self.code_snippets:
            return []

        query_vector = self._embed(query)
        # First get more candidates than needed
        indices, distances = self.index.knn_query(query_vector, k=min(top_k*3, len(self.code_snippets)))

        results = []
        for idx, dist in zip(indices[0], distances[0]):
            # Apply filters
            if language_filter and self.metadata[idx]["language"] != language_filter:
                continue
            if category_filter and self.metadata[idx].get("category") != category_filter:
                continue

            combined_score = self.get_combined_score(idx)
            adjusted_distance = dist * (1.1 - combined_score)  # Better scores reduce distance

            results.append({
                "id": idx,
                "code": self.code_snippets[idx],
                "source": self.metadata[idx]["source"],
                "language": self.metadata[idx]["language"],
                "category": self.metadata[idx].get("category", ""),
                "cosine_distance": float(dist),
                "feedback_score": self.feedback_scores.get(idx, 0),
                "usage_count": self.usage_counts.get(idx, 0),
                "combined_score": combined_score,
                "adjusted_distance": adjusted_distance,
                "last_used": self.last_used.get(idx, datetime.min).isoformat()
            })

        # Sort by adjusted distance and take top_k
        results = sorted(results, key=lambda x: x["adjusted_distance"])[:top_k]

        # Update usage counts for returned results
        for result in results:
            self.increment_usage(result["id"])

        return results

    def get_top_items(self, language: str, category: Optional[str] = None,
                     top_k: int = 5) -> List[Dict]:
        """Get top items based on combined score, not semantic similarity"""
        all_items = []
        for idx in range(len(self.code_snippets)):
            if language and self.metadata[idx]["language"] != language:
                continue
            if category and self.metadata[idx].get("category") != category:
                continue

            all_items.append({
                "id": idx,
                "code": self.code_snippets[idx],
                "source": self.metadata[idx]["source"],
                "language": self.metadata[idx]["language"],
                "category": self.metadata[idx].get("category", ""),
                "feedback_score": self.feedback_scores.get(idx, 0),
                "usage_count": self.usage_counts.get(idx, 0),
                "combined_score": self.get_combined_score(idx),
                "last_used": self.last_used.get(idx, datetime.min).isoformat()
            })

        return sorted(all_items, key=lambda x: x["combined_score"], reverse=True)[:top_k]

    def _embed(self, text: str) -> np.ndarray:
        return embedding_model.encode(text)

# --- Enhanced Code Memory System ---
class CodeMemory:
    def __init__(self):
        self.conn = sqlite3.connect(Config.HISTORY_DB, check_same_thread=False)
        self._init_db()

    def _init_db(self):
        cursor = self.conn.cursor()
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS code_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT NOT NULL,
            generated_code TEXT NOT NULL,
            language TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            feedback_score REAL,
            feedback_notes TEXT,
            execution_result TEXT,
            lint_result TEXT,
            retrieval_rank INTEGER,
            is_favorite BOOLEAN DEFAULT 0,
            category TEXT,
            context_hash TEXT
        )""")

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS project_context (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            project_hash TEXT NOT NULL,
            file_path TEXT NOT NULL,
            content TEXT NOT NULL,
            embeddings BLOB,
            last_accessed DATETIME DEFAULT CURRENT_TIMESTAMP,
            feedback_score REAL DEFAULT 1.0,
            usage_count INTEGER DEFAULT 0,
            category TEXT
        )""")

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS knowledge_base (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            content TEXT NOT NULL,
            language TEXT NOT NULL,
            source TEXT,
            category TEXT,
            last_accessed DATETIME DEFAULT CURRENT_TIMESTAMP,
            feedback_score REAL DEFAULT 1.0,
            usage_count INTEGER DEFAULT 0,
            UNIQUE(content, language)
        )""")

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS feedback_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            history_id INTEGER,
            feedback_score REAL NOT NULL,
            feedback_notes TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(history_id) REFERENCES code_history(id)
        )""")

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS context_associations (
            history_id INTEGER,
            context_id INTEGER,
            association_type TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(history_id) REFERENCES code_history(id),
            FOREIGN KEY(context_id) REFERENCES code_history(id)
        )""")

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS function_calls (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            history_id INTEGER,
            tool_name TEXT NOT NULL,
            parameters TEXT NOT NULL,
            result TEXT,
            success BOOLEAN,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(history_id) REFERENCES code_history(id)
        )""")
        self.conn.commit()

    def add_history(self, query: str, generated_code: str, language: str,
                   feedback_score: Optional[float] = None, feedback_notes: Optional[str] = None,
                   execution_result: Optional[str] = None, lint_result: Optional[str] = None,
                   retrieval_rank: Optional[int] = None, is_favorite: bool = False,
                   category: Optional[str] = None, context_hash: Optional[str] = None):
        cursor = self.conn.cursor()
        cursor.execute("""
        INSERT INTO code_history
        (query, generated_code, language, feedback_score, feedback_notes,
         execution_result, lint_result, retrieval_rank, is_favorite, category, context_hash)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (query, generated_code, language, feedback_score, feedback_notes,
              execution_result, lint_result, retrieval_rank, is_favorite, category, context_hash))
        self.conn.commit()
        return cursor.lastrowid

    def log_function_call(self, history_id: int, tool_name: str, parameters: Dict,
                         result: Optional[str] = None, success: bool = True):
        cursor = self.conn.cursor()
        cursor.execute("""
        INSERT INTO function_calls
        (history_id, tool_name, parameters, result, success)
        VALUES (?, ?, ?, ?, ?)
        """, (history_id, tool_name, json.dumps(parameters), json.dumps(result), success))
        self.conn.commit()
        return cursor.lastrowid

    def update_feedback(self, history_id: int, feedback_score: float, feedback_notes: str = ""):
        cursor = self.conn.cursor()
        try:
            # Update the history record
            cursor.execute("""
            UPDATE code_history
            SET feedback_score = ?, feedback_notes = ?
            WHERE id = ?
            """, (feedback_score, feedback_notes, history_id))

            # Log the feedback separately
            cursor.execute("""
            INSERT INTO feedback_log (history_id, feedback_score, feedback_notes)
            VALUES (?, ?, ?)
            """, (history_id, feedback_score, feedback_notes))

            self.conn.commit()
            return True
        except sqlite3.Error as e:
            logger.error(f"Error updating feedback: {e}")
            return False

    def toggle_favorite(self, history_id: int) -> bool:
        cursor = self.conn.cursor()
        try:
            cursor.execute("""
            UPDATE code_history
            SET is_favorite = NOT is_favorite
            WHERE id = ?
            """, (history_id,))
            self.conn.commit()
            return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"Error toggling favorite: {e}")
            return False

    def get_recent_history(self, limit: int = 5, language: Optional[str] = None,
                          only_favorites: bool = False, category: Optional[str] = None) -> List[Dict]:
        cursor = self.conn.cursor()
        query = """
        SELECT id, query, generated_code, language, timestamp,
               feedback_score, is_favorite, retrieval_rank, category
        FROM code_history
        """
        params = []

        conditions = []
        if language:
            conditions.append("language = ?")
            params.append(language)
        if only_favorites:
            conditions.append("is_favorite = 1")
        if category:
            conditions.append("category = ?")
            params.append(category)

        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        query += " ORDER BY timestamp DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        columns = [column[0] for column in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]

    def get_best_history(self, language: str, limit: int = 5, category: Optional[str] = None) -> List[Dict]:
        """Get highest-rated historical items based on feedback and usage"""
        cursor = self.conn.cursor()
        query = """
        SELECT id, query, generated_code, language, timestamp,
               feedback_score, is_favorite, retrieval_rank, category
        FROM code_history
        WHERE language = ? AND feedback_score IS NOT NULL
        """
        params = [language]

        if category:
            query += " AND category = ?"
            params.append(category)

        query += """
        ORDER BY feedback_score DESC, timestamp DESC
        LIMIT ?
        """
        params.append(limit)

        cursor.execute(query, params)
        columns = [column[0] for column in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]

    def get_context_window(self, history_id: int, window_size: int = Config.CONTEXT_WINDOW) -> List[Dict]:
        """Get related historical items that provide context for the given history item"""
        cursor = self.conn.cursor()

        # First get the target item
        cursor.execute("""
        SELECT context_hash FROM code_history WHERE id = ?
        """, (history_id,))
        result = cursor.fetchone()
        if not result:
            return []

        context_hash = result[0]
        if not context_hash:
            return []

        # Get items with the same context hash
        cursor.execute("""
        SELECT id, query, generated_code, language, timestamp,
               feedback_score, is_favorite, retrieval_rank, category
        FROM code_history
        WHERE context_hash = ? AND id != ?
        ORDER BY timestamp DESC
        LIMIT ?
        """, (context_hash, history_id, window_size))

        columns = [column[0] for column in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]

    def add_knowledge(self, content: str, language: str, source: str = "", category: str = "") -> bool:
        cursor = self.conn.cursor()
        try:
            cursor.execute("""
            INSERT OR IGNORE INTO knowledge_base
            (content, language, source, category)
            VALUES (?, ?, ?, ?)
            """, (content, language, source, category))
            self.conn.commit()
            return True
        except sqlite3.Error as e:
            logger.error(f"Error adding knowledge: {e}")
            return False

    def update_knowledge_feedback(self, knowledge_id: int, feedback_score: float) -> bool:
        cursor = self.conn.cursor()
        try:
            # Apply decay to existing feedback
            cursor.execute("""
            UPDATE knowledge_base
            SET feedback_score = feedback_score * ? + ? * (1 - ?),
                last_accessed = CURRENT_TIMESTAMP
            WHERE id = ?
            """, (Config.DECAY_FACTOR, feedback_score, Config.DECAY_FACTOR, knowledge_id))
            self.conn.commit()
            return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"Error updating knowledge feedback: {e}")
            return False

    def increment_knowledge_usage(self, knowledge_id: int) -> bool:
        cursor = self.conn.cursor()
        try:
            cursor.execute("""
            UPDATE knowledge_base
            SET usage_count = usage_count + 1,
                last_accessed = CURRENT_TIMESTAMP
            WHERE id = ?
            """, (knowledge_id,))
            self.conn.commit()
            return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"Error incrementing knowledge usage: {e}")
            return False

    def get_knowledge(self, language: str, category: Optional[str] = None,
                     limit: int = Config.KNOWLEDGE_BASE_LIMIT) -> List[Dict]:
        cursor = self.conn.cursor()
        try:
            # First get max usage count for normalization
            cursor.execute("""
            SELECT MAX(usage_count) FROM knowledge_base WHERE language = ?
            """, (language,))
            max_usage = cursor.fetchone()[0] or 1

            if category:
                cursor.execute("""
                SELECT id, content, source, category, last_accessed,
                       feedback_score, usage_count,
                       (feedback_score * ? + (usage_count / ?) * ?) AS combined_score
                FROM knowledge_base
                WHERE language = ? AND category = ?
                ORDER BY combined_score DESC, last_accessed DESC
                LIMIT ?
                """, (Config.FEEDBACK_WEIGHT, max_usage, Config.USAGE_WEIGHT,
                     language, category, limit))
            else:
                cursor.execute("""
                SELECT id, content, source, category, last_accessed,
                       feedback_score, usage_count,
                       (feedback_score * ? + (usage_count / ?) * ?) AS combined_score
                FROM knowledge_base
                WHERE language = ?
                ORDER BY combined_score DESC, last_accessed DESC
                LIMIT ?
                """, (Config.FEEDBACK_WEIGHT, max_usage, Config.USAGE_WEIGHT,
                     language, limit))

            columns = [column[0] for column in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
        except sqlite3.Error as e:
            logger.error(f"SQL Error in get_knowledge: {e}")
            raise
        except Exception as e:
            logger.error(f"Error in get_knowledge: {e}")
            raise

    def add_project_file(self, project_hash: str, file_path: str, content: str, category: Optional[str] = None):
        cursor = self.conn.cursor()
        cursor.execute("""
        INSERT OR REPLACE INTO project_context
        (project_hash, file_path, content, category)
        VALUES (?, ?, ?, ?)
        """, (project_hash, file_path, content, category))
        self.conn.commit()

    def get_project_files(self, project_hash: str) -> List[Dict]:
        cursor = self.conn.cursor()
        cursor.execute("""
        SELECT file_path, content, category
        FROM project_context
        WHERE project_hash = ?
        ORDER BY last_accessed DESC
        """, (project_hash,))
        columns = [column[0] for column in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]

    def associate_context(self, history_id: int, context_ids: List[int], association_type: str = "related"):
        cursor = self.conn.cursor()
        try:
            for ctx_id in context_ids:
                cursor.execute("""
                INSERT OR IGNORE INTO context_associations
                (history_id, context_id, association_type)
                VALUES (?, ?, ?)
                """, (history_id, ctx_id, association_type))
            self.conn.commit()
            return True
        except sqlite3.Error as e:
            logger.error(f"Error associating context: {e}")
            return False

# --- Code Execution System ---
class CodeExecutor:
    def __init__(self):
        self.docker_client = None
        if USE_DOCKER:
            try:
                import docker
                self.docker_client = docker.from_env()
                self.docker_client.ping()
            except Exception as e:
                logger.warning(f"Docker initialization failed: {str(e)}")
                logger.warning("Code execution will be limited - Docker functionality unavailable")
                self.docker_client = None

    def _check_docker(self) -> bool:
        if self.docker_client is None:
            return False
        try:
            return self.docker_client.ping()
        except:
            return False

    def execute_code(self, code: str, language: str) -> Dict:
        if not USE_DOCKER:
            return {"error": "Docker execution disabled in Colab", "output": "Execution disabled in Colab environment"}

        if not self.docker_client:
            return {"error": "Docker not available"}

        try:
            lang_config = Config.SUPPORTED_LANGUAGES.get(language)
            if not lang_config:
                return {"error": f"Unsupported language: {language}"}

            container_name = f"exec_{hashlib.md5(code.encode()).hexdigest()[:8]}"

            if language in ["javascript", "typescript", "react", "tailwind", "vite", "nestjs"]:
                cmd = ["node", "-e", code]
            elif language == "python":
                cmd = ["python", "-c", code]
            elif language == "java":
                return {"error": "Java execution requires file compilation"}
            else:
                return {"error": f"Execution not configured for {language}"}

            container = self.docker_client.containers.run(
                lang_config["docker_image"],
                command=cmd,
                name=container_name,
                detach=True,
                mem_limit="100m",
                cpu_period=50000,
                cpu_quota=25000,
                network_mode="none"
            )

            try:
                result = container.wait(timeout=Config.DOCKER_TIMEOUT)
                output = container.logs(stdout=True, stderr=True).decode("utf-8")[:Config.MAX_OUTPUT_LENGTH]

                return {
                    "exit_code": result["StatusCode"],
                    "output": output,
                    "error": None
                }
            finally:
                container.remove(force=True)
        except Exception as e:
            return {"error": str(e)}

    def lint_code(self, code: str, language: str) -> Dict:
        lang_config = Config.SUPPORTED_LANGUAGES.get(language)
        if not lang_config or not lang_config["linter"]:
            return {"error": f"No linter configured for {language}"}

        try:
            temp_file = Path(f"temp{lang_config['ext']}")
            temp_file.write_text(code)

            if language == "python":
                result = subprocess.run(
                    ["flake8", str(temp_file)],
                    capture_output=True,
                    text=True
                )
            elif language in ["javascript", "typescript", "react", "vite", "nestjs"]:
                result = subprocess.run(
                    ["eslint", str(temp_file)],
                    capture_output=True,
                    text=True
                )
            else:
                return {"error": f"Linting not implemented for {language}"}

            temp_file.unlink()
            return {
                "success": result.returncode == 0,
                "output": result.stdout or result.stderr
            }
        except Exception as e:
            return {"error": str(e)}

# --- Model Loading with GPU Optimization ---
def load_model_with_gpu():
    try:
        logger.info(f"Loading model: {Config.MODEL_NAME} with GPU acceleration")

        # First try loading with device_map="auto" for automatic GPU allocation
        try:
            model = AutoModelForCausalLM.from_pretrained(
                Config.MODEL_NAME,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            logger.info("Model loaded with automatic GPU allocation")
            return model
        except Exception as e:
            logger.warning(f"Automatic GPU allocation failed, trying manual loading: {e}")

        # If automatic fails, try manual GPU loading
        if torch.cuda.is_available():
            device = torch.device("cuda")
            model = AutoModelForCausalLM.from_pretrained(
                Config.MODEL_NAME,
                torch_dtype=torch.float16
            ).to(device)
            logger.info("Model manually loaded to GPU")
            return model

        # Fallback to CPU if GPU not available
        logger.warning("No GPU available, falling back to CPU")
        return AutoModelForCausalLM.from_pretrained(
            Config.MODEL_NAME,
            torch_dtype=torch.float32
        )
    except Exception as e:
        logger.error(f"Model loading failed: {e}")
        raise RuntimeError(f"Could not load model: {str(e)}")

# --- Enhanced Code Agent with Feedback and Function Calling Integration ---
class FeedbackDrivenCodeAgent:
    def __init__(self, retriever, memory, executor, tokenizer, model):
        """
        Enhanced code generation agent with:
        - Feedback-aware retrieval
        - Streaming generation
        - Function calling
        - Comprehensive error handling
        """
        self.retriever = retriever
        self.memory = memory
        self.executor = executor
        self.tokenizer = tokenizer
        self.model = model
        self.function_caller = FunctionCallingSystem(executor, memory, retriever)
        self.param_tuner = GenerationParameterTuner(self)
        self.auto_optimize_enabled = True

        # Initialize stopping criteria for function calls
        self.stopping_criteria = StoppingCriteriaList([FunctionCallStopper(tokenizer)])

    async def generate_code_stream(
        self,
        task_description: str,
        language: str = "python",
        context: Optional[str] = None,
        project_hash: Optional[str] = None,
        category: Optional[str] = None
    ) -> StreamingResponse:
        """
        Main endpoint for streaming code generation with:
        - Context-aware prompts
        - Knowledge retrieval
        - Error resiliency
        """
        try:
            # Input validation
            if language not in Config.SUPPORTED_LANGUAGES:
                raise HTTPException(status_code=400, detail=f"Unsupported language: {language}")

            # Context building with error resilience
            context_data = await self._build_generation_context(
                task_description, language, context, project_hash, category
            )

            return StreamingResponse(
                self._generate_with_error_handling(**context_data),
                media_type="text/plain"
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Generation setup failed: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail="Initialization error")

    async def _build_generation_context(
        self,
        task: str,
        language: str,
        context: Optional[str],
        project_hash: Optional[str],
        category: Optional[str]
    ) -> Dict:
        """Safely build all context components with isolated error handling"""
        try:
            project_files = []
            if project_hash:
                project_files = self.memory.get_project_files(project_hash)

            # Retrieve knowledge with fallback
            try:
                knowledge_items = self.memory.get_knowledge(language=language, category=category)
                knowledge_str = self._format_knowledge(knowledge_items)
            except Exception as e:
                knowledge_str = f"// Knowledge retrieval failed: {str(e)}"
                logger.warning(f"Knowledge retrieval failed: {str(e)}")

            # Retrieve history with fallback
            try:
                best_history = self.memory.get_best_history(language=language, category=category)
                history_str = self._format_history(best_history)
            except Exception as e:
                history_str = f"// History retrieval failed: {str(e)}"
                logger.warning(f"History retrieval failed: {str(e)}")

            # Generate context hash for tracking
            context_hash = hashlib.md5(
                task + language + (category or "") + (project_hash or "")).hexdigest()

            prompt = self._build_prompt(
                task, language, context,
                knowledge_str, history_str,
                project_files, category
            )

            return {
                "prompt": prompt,
                "task": task,
                "language": language,
                "context_hash": context_hash,
                "category": category
            }

        except Exception as e:
            logger.error(f"Context building failed: {str(e)}", exc_info=True)
            raise

    def _post_generation_optimization(self):
        """Run periodic optimization tasks"""
        if self.auto_optimize_enabled:
            try:
                # Optimize retriever architecture
                self.retriever.auto_optimize()

                # Tune generation parameters
                self.param_tuner.optimize()

                logger.info("Completed post-generation optimization")
            except Exception as e:
                logger.error(f"Optimization error: {str(e)}")

    async def _generate_with_error_handling(
        self,
        prompt: str,
        task: str,
        language: str,
        context_hash: str,
        category: Optional[str] = None
    ):
        """
        Core generation loop with:
        - Token streaming
        - Function call interception
        - Comprehensive error recovery
        """
        full_response = ""
        history_id = None
        function_call_attempts = 0
        device = self._get_available_device()

        try:
            # Initialize generation
            inputs = await self._safe_tokenize(prompt, device)
            if inputs is None:
                yield "// Tokenization failed - check your input"
                return

            # Main generation loop
            while True:
                try:
                    # Generate stream of tokens
                    async for token_output in self._stream_tokens(inputs, device):
                        if token_output.get('error'):
                            yield token_output['error']
                            continue

                        text = token_output['text']
                        full_response += text
                        yield text

                        # Check for early termination
                        if token_output.get('should_stop'):
                            break

                    # Handle function calls
                    function_call = self.function_caller.detect_function_call(full_response)
                    if not function_call:
                        break  # Normal completion

                    # Process function call
                    func_result = await self._handle_function_call(
                        function_call, history_id, full_response
                    )
                    if func_result.get('error'):
                        yield func_result['error']
                        break

                    # Update for next generation round
                    function_call_attempts += 1
                    if function_call_attempts >= Config.MAX_FUNCTION_CALL_ATTEMPTS:
                        yield "\n// Maximum function calls reached"
                        break

                    inputs = await self._prepare_next_input(
                        prompt, full_response, func_result['response'], device
                    )
                    full_response = ""

                except torch.cuda.OutOfMemoryError:
                    yield "\n// GPU out of memory - switching to CPU"
                    device = "cpu"
                    inputs = inputs.to(device)
                    continue
                except Exception as e:
                    yield f"\n// Generation error: {str(e)}"
                    logger.error(f"Generation error: {str(e)}", exc_info=True)
                    break

            # Post-generation processing
            yield self._finalize_generation(
                full_response, task, language, context_hash, category
            )

        except Exception as e:
            yield f"\n// Critical error: {str(e)}"
            logger.critical(f"Critical generation error: {str(e)}", exc_info=True)

    async def _stream_tokens(self, inputs, device):
        """Token streaming generator with error resilience"""
        try:
            for token in self.model.generate(
                **inputs,
                **Config.GENERATION_CONFIG,
                stopping_criteria=self.stopping_criteria,
                pad_token_id=self.tokenizer.eos_token_id
            ):
                try:
                    text = self.tokenizer.decode(token, skip_special_tokens=True)
                    yield {
                        'text': text,
                        'should_stop': self._should_stop_generation(text)
                    }
                except Exception as e:
                    yield {'error': f"\n// Token decoding failed: {str(e)}"}
        except Exception as e:
            yield {'error': f"\n// Token generation failed: {str(e)}"}

    async def _handle_function_call(self, function_call, history_id, full_response):
        """Execute function call with full error handling"""
        try:
            tool_name = function_call["tool"]
            params = function_call["parameters"]

            # Execute tool
            result = await self.function_caller.call_tool(tool_name, params)

            # Log if we have history context
            if history_id:
                self.memory.log_function_call(
                    history_id,
                    tool_name,
                    params,
                    result.get("result"),
                    result["success"]
                )

            return {
                'response': result,
                'tool': tool_name
            }
        except Exception as e:
            return {
                'error': f"\n// Tool execution failed: {str(e)}"
            }

    async def _finalize_generation(self, code, task, language, context_hash, category):
       """Validate and store generated code"""
       try:
          if not self._validate_code(code, language):
              yield "\n// Validation failed - no valid code patterns detected"
              return

        # Store in history
          history_id = self.memory.add_history(
              task,
              code,
              language,
              category=category,
              context_hash=context_hash
          )

        # Lint the code
          lint_result = self.executor.lint_code(code, language)
          if not lint_result.get("success"):
              yield f"\n// Lint warnings:\n{lint_result.get('output', '')}"

          yield f"\n// Generation complete (ID: {history_id})"

       except Exception as e:
            yield f"\n// Storage failed: {str(e)}"
            logger.error(f"Post-generation processing failed: {str(e)}", exc_info=True)

    # Helper methods
    def _get_available_device(self):
        return "cuda" if torch.cuda.is_available() else "cpu"

    async def _safe_tokenize(self, text, device):
        try:
            return self.tokenizer(text, return_tensors="pt").to(device)
        except Exception as e:
            logger.error(f"Tokenization failed: {str(e)}", exc_info=True)
            return None

    def _should_stop_generation(self, text):
        return Config.FUNCTION_CALL_PREFIX in text

    async def _prepare_next_input(self, original_prompt, generated_text, tool_result, device):
        continuation_prompt = (
            f"{original_prompt}{generated_text}\n"
            f"// Tool result:\n{json.dumps(tool_result, indent=2)[:Config.FUNCTION_CALL_TOKEN_LIMIT]}\n"
            "// Continue generation:"
        )
        return await self._safe_tokenize(continuation_prompt, device)

    def _validate_code(self, code, language):
        patterns = Config.SUPPORTED_LANGUAGES[language].get("validation_patterns", [])
        return any(re.search(pattern, code) for pattern in patterns)

    def _format_knowledge(self, items):
        return "\n".join(
            f"// Knowledge ({item.get('category', 'general')}):\n{item['content']}"
            for item in items[:Config.KNOWLEDGE_BASE_LIMIT]
        )

    def _format_history(self, items):
        return "\n".join(
            f"// Example {i+1}:\n{item['generated_code']}"
            for i, item in enumerate(items[:3])
        )

    def _build_prompt(self, task, language, context, knowledge, history, project_files, category):
        lang_config = Config.SUPPORTED_LANGUAGES[language]
        return f"""```{lang_config['ext']}
// Task: {task}
// Language: {language}
// Context: {context or 'None'}

// Knowledge Base:
{knowledge or '// No knowledge available'}

// Historical Examples:
{history or '// No history available'}

// Project Files:
{self._format_project_files(project_files) if project_files else '// No project files'}

// Instructions:
{self._get_instructions(language, category)}

// Generated Code:
```"""

class FunctionCallStopper(StoppingCriteria):
    """Custom stopping criteria for function call detection"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        last_tokens = input_ids[0][-20:]  # Look at last 20 tokens
        decoded = self.tokenizer.decode(last_tokens)
        return Config.FUNCTION_CALL_PREFIX in decoded

# --- Repository Scraping and Initialization ---
REPOSITORY_URLS = {
    "react": "https://github.com/facebook/react",
    "javascript": "https://github.com/mdn/content",
    "typescript": "https://github.com/microsoft/TypeScript",
    "tailwind": "https://github.com/tailwindlabs/tailwindcss",
    "vite": "https://github.com/vitejs/vite",
    "nestjs": "https://github.com/nestjs/nest",
    "python": "https://github.com/python/cpython",
    "java": "https://github.com/spring-projects/spring-framework"
}


def optimization_worker():
    while True:
        try:
            # Only run optimization if there are enough items
            if len(retriever.code_snippets) > 100:
                retriever.architecture_optimizer.optimize()
                logger.info("Completed retriever optimization")

            if agent.auto_optimize_enabled:
                agent.param_tuner.optimize()
                logger.info("Completed generation parameter tuning")

        except Exception as e:
            logger.error(f"Optimization error: {str(e)}")
        time.sleep(3600)  # Run hourly


def initialize_system():
    logger.info("Initializing system components...")

    global embedding_model
    embedding_model = SentenceTransformer(
        Config.EMBEDDING_MODEL,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    global retriever
    retriever = FeedbackAwareRetriever()
    retriever.architecture_optimizer = ArchitectureOptimizer(retriever)

    # Initialize code memory with feedback tracking
    global memory
    memory = CodeMemory()

    # Initialize code executor
    global executor
    executor = CodeExecutor()

    # Load the model with GPU optimization
    global model, tokenizer
    try:
        logger.info("Loading model with GPU optimization...")
        model = load_model_with_gpu()
        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)

        # Test the model
        try:
            test_inputs = tokenizer("Test input", return_tensors="pt").to(model.device)
            test_output = model.generate(**test_inputs, max_new_tokens=1)
            logger.info(f"Model test successful (device: {model.device})")
        except Exception as e:
            logger.error(f"Model test failed: {e}")
            raise
    except Exception as e:
        logger.error(f"Failed to initialize model: {e}")
        raise

    # Initialize the enhanced agent
    global agent
    agent = FeedbackDrivenCodeAgent(retriever, memory, executor, tokenizer, model)
    agent.auto_optimize_enabled = True
    threading.Thread(target=optimization_worker, daemon=True).start()

    logger.info("System initialization complete")

# --- API Endpoints ---
class KnowledgeItem(BaseModel):
    content: str
    language: str
    source: Optional[str] = None
    category: Optional[str] = None

class FeedbackRequest(BaseModel):
    history_id: int
    score: float  # Between 0 (bad) and 1 (good)
    notes: Optional[str] = None

class FavoriteRequest(BaseModel):
    history_id: int

class GenerateCodeRequest(BaseModel):
    task_description: str
    language: str = "python"
    context: Optional[str] = None
    project_hash: Optional[str] = None
    category: Optional[str] = None

class ExecuteCodeRequest(BaseModel):
    code: str
    language: str
    feedback_score: Optional[float] = None

class UploadProjectRequest(BaseModel):
    project_hash: str
    file_path: str
    content: str
    category: Optional[str] = None


@app.post("/optimize/retriever")
async def optimize_retriever(n_trials: int = 50):
    retriever.architecture_optimizer.optimize_retriever(n_trials)
    return {"status": "Retriever architecture optimized"}

@app.post("/optimize/generation")
async def optimize_generation(n_trials: int = 100):
    agent.param_tuner.optimize(n_trials)
    return {"status": "Generation parameters optimized"}

@app.post("/knowledge/add")
async def add_knowledge(item: KnowledgeItem):
    try:
        success = memory.add_knowledge(
            content=item.content,
            language=item.language,
            source=item.source,
            category=item.category
        )
        if not success:
            raise HTTPException(status_code=400, detail="Failed to add knowledge item")
        return {"status": "success"}
    except Exception as e:
        logger.error(f"Error adding knowledge: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/knowledge/get/{language}")
async def get_knowledge(
    language: str,
    category: Optional[str] = None,
    limit: int = Config.KNOWLEDGE_BASE_LIMIT
):
    try:
        items = memory.get_knowledge(language, category, limit)
        return {"items": items}
    except Exception as e:
        logger.error(f"Error getting knowledge: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/knowledge/delete/{knowledge_id}")
async def delete_knowledge(knowledge_id: int):
    try:
        cursor = memory.conn.cursor()
        cursor.execute("DELETE FROM knowledge_base WHERE id = ?", (knowledge_id,))
        memory.conn.commit()
        if cursor.rowcount == 0:
            raise HTTPException(status_code=404, detail="Knowledge item not found")
        return {"status": "success"}
    except Exception as e:
        logger.error(f"Error deleting knowledge: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    try:
        if not 0 <= request.score <= 1:
            raise HTTPException(status_code=400, detail="Score must be between 0 and 1")

        # Update in memory database
        success = memory.update_feedback(
            request.history_id,
            request.score,
            request.notes or ""
        )

        if not success:
            raise HTTPException(status_code=404, detail="History item not found")

        return {"status": "success"}
    except Exception as e:
        logger.error(f"Error submitting feedback: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/favorite")
async def toggle_favorite(request: FavoriteRequest):
    try:
        success = memory.toggle_favorite(request.history_id)
        if not success:
            raise HTTPException(status_code=404, detail="History item not found")
        return {"status": "success"}
    except Exception as e:
        logger.error(f"Error toggling favorite: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/history/recent")
async def get_recent_history(
    language: Optional[str] = None,
    limit: int = 5,
    only_favorites: bool = False,
    category: Optional[str] = None
):
    try:
        items = memory.get_recent_history(limit, language, only_favorites, category)
        return {"items": items}
    except Exception as e:
        logger.error(f"Error getting recent history: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/history/best/{language}")
async def get_best_history(
    language: str,
    limit: int = 5,
    category: Optional[str] = None
):
    try:
        items = memory.get_best_history(language, limit, category)
        return {"items": items}
    except Exception as e:
        logger.error(f"Error getting best history: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/history/context/{history_id}")
async def get_context_window(
    history_id: int,
    window_size: int = Config.CONTEXT_WINDOW
):
    try:
        items = memory.get_context_window(history_id, window_size)
        return {"items": items}
    except Exception as e:
        logger.error(f"Error getting context window: {e}")
        raise HTTPException(status_code=500, detail=str(e))



@app.post("/generate_code")
async def generate_code(request: GenerateCodeRequest):
    try:
        if request.language not in Config.SUPPORTED_LANGUAGES:
            raise HTTPException(status_code=400, detail=f"Unsupported language: {request.language}")

        logger.info(f"Generating code for: {request.task_description[:100]}...")

        # Initialize response
        full_response = ""

        async for chunk in agent.generate_code_stream(
            task_description=request.task_description,
            language=request.language,
            context=request.context,
            project_hash=request.project_hash,
            category=request.category
        ):
            full_response += chunk
            yield chunk

        logger.info(f"Generation completed successfully")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generation failed: {str(e)}", exc_info=True)
        yield f"\n// Generation error: {str(e)}"


@app.post("/execute_code")
async def execute_code(request: ExecuteCodeRequest):
    try:
        return await agent.execute_and_feedback(
            request.code,
            request.language,
            request.feedback_score
        )
    except Exception as e:
        logger.error(f"Error in /execute_code: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload_project_file")
async def upload_project_file(request: UploadProjectRequest):
    try:
        # Validate file path
        if not re.match(r"^[\w\-/]+\.\w+$", request.file_path):
            raise HTTPException(status_code=400, detail="Invalid file path")

        # Limit file size
        if len(request.content) > 100000:  # 100KB
            raise HTTPException(status_code=413, detail="File too large")

        memory.add_project_file(
            request.project_hash,
            request.file_path,
            request.content,
            request.category
        )
        return {"status": "success"}
    except Exception as e:
        logger.error(f"Error uploading project file: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/project_files/{project_hash}")
async def get_project_files(project_hash: str):
    try:
        return memory.get_project_files(project_hash)
    except Exception as e:
        logger.error(f"Error getting project files: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/scrape_repos")
async def scrape_repos():
    scraped_code = {}
    for lang, url in REPOSITORY_URLS.items():
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            code_blocks = [code.text for code in soup.find_all('code')]
            if code_blocks:
                scraped_code[lang] = len(code_blocks)
                for code in code_blocks[:100]:  # Limit to 100 snippets per repo
                    # Try to determine category from code
                    category = None
                    if lang == "python":
                        if re.search(r"def\s+train_|model\.fit", code):
                            category = "ml"
                        elif re.search(r"Flask|Django|FastAPI", code):
                            category = "web"
                    elif lang == "javascript":
                        if re.search(r"express\(|\.get\(", code):
                            category = "server"
                        elif re.search(r"document\.|window\.", code):
                            category = "frontend"

                    retriever.add_code(
                        code,
                        source=url,
                        language=lang,
                        category=category
                    )
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}")
    return {"scraped_code": scraped_code}

@app.get("/retriever/top/{language}")
async def get_top_retrieved(language: str, category: Optional[str] = None, top_k: int = 5):
    try:
        items = retriever.get_top_items(language, category, top_k)
        return {"items": items}
    except Exception as e:
        logger.error(f"Error getting top retrieved items: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/supported_languages")
async def get_supported_languages():
    return {
        "supported_languages": list(Config.SUPPORTED_LANGUAGES.keys()),
        "details": {
            lang: {
                "ext": info["ext"],
                "categories": info.get("categories", []),
                "linter": info["linter"]
            }
            for lang, info in Config.SUPPORTED_LANGUAGES.items()
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "retriever_items": len(retriever.code_snippets),
        "memory_stats": {
            "history_items": memory.conn.cursor().execute("SELECT COUNT(*) FROM code_history").fetchone()[0],
            "knowledge_items": memory.conn.cursor().execute("SELECT COUNT(*) FROM knowledge_base").fetchone()[0],
            "function_calls": memory.conn.cursor().execute("SELECT COUNT(*) FROM function_calls").fetchone()[0]
        }
    }

@app.get("/")
def read_root():
    return {"message": "Feedback-Driven Code Generator with Function Calling (DeepSeek-Coder-6.7B-Instruct)"}

@app.post("/analyze")
async def analyze_code(request: Request):
    data = await request.json()
    # Add your analysis logic here
    return {"analysis": "Your analysis results"}

# --- Colab Setup Function ---
def run_in_colab():
    from pyngrok import ngrok
    import threading

    ngrok.set_auth_token("2twDPe74r44YB1AZZT6xfOYZn7Z_7q2AqyPZPixSYvQ5m44h")

    def start_server():
        uvicorn.run(app, host="0.0.0.0", port=8000)  # Try a different port

    server_thread = threading.Thread(target=start_server, daemon=True)
    server_thread.start()

    public_url = ngrok.connect(8000).public_url
    print(f" * ngrok tunnel \"{public_url}\" -> \"http://127.0.0.1:8000\"")
    return public_url

# --- Main Execution ---
if __name__ == "__main__":
    # Check if we're running in Colab
    try:
        from google.colab import drive
        IN_COLAB = True
    except:
        IN_COLAB = False

    # Initialize the system
    initialize_system()

    if IN_COLAB:
        print("Running in Google Colab - setting up ngrok tunnel")
        public_url = run_in_colab()
        print(f"Server is running at: {public_url}")
        print("\nTest the API with these commands:")
        print(f"""
# Generate Python code
!curl -X POST "{public_url}/generate_code" \\
  -H "Content-Type: application/json" \\
  -d '{{"task_description": "Create a function to calculate factorial", "language": "python"}}'

# Execute Python code with feedback
!curl -X POST "{public_url}/execute_code" \\
  -H "Content-Type: application/json" \\
  -d '{{"code": "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)", "language": "python", "feedback_score": 0.9}}'

# Get supported languages
!curl "{public_url}/supported_languages"

# Get best Python examples
!curl "{public_url}/history/best/python"
        """)

        # Keep the server running indefinitely
        try:
            while True:
                pass
        except KeyboardInterrupt:
            print("\nServer shutting down...")
    else:
        # Normal execution outside Colab
        try:
            # Add resource limits
            import resource
            resource.setrlimit(resource.RLIMIT_AS, (5 * 1024**3, 5 * 1024**3))  # 5GB memory limit

            # Run the server
            uvicorn.run(
                app,
                host="0.0.0.0",
                port=8080,
                workers=1,
                limit_concurrency=1,
                timeout_keep_alive=30,
                loop="asyncio",
                reload=False
            )
        except KeyboardInterrupt:
            logger.info("Server stopped by user")
        except Exception as e:
            logger.error(f"Server crashed: {e}")
            raise

